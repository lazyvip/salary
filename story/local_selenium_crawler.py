#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬åœ°Seleniumçˆ¬è™« - ä½¿ç”¨ç³»ç»Ÿå·²å®‰è£…çš„Chromeæµè§ˆå™¨
å¤„ç†JavaScriptæ¸²æŸ“çš„æ•…äº‹ç½‘ç«™
"""

import time
import json
import sqlite3
import logging
import re
import os
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from bs4 import BeautifulSoup
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed

class LocalSeleniumCrawler:
    def __init__(self, max_workers=1, headless=False):
        self.max_workers = max_workers
        self.headless = headless
        self.base_url = "https://storynook.cn"
        self.stories = []
        self.failed_ids = []
        self.success_count = 0
        self.total_count = 0
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('local_selenium.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.init_database()
        
    def find_chrome_executable(self):
        """æŸ¥æ‰¾Chromeå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„"""
        possible_paths = [
            r"C:\Program Files\Google\Chrome\Application\chrome.exe",
            r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe",
            r"C:\Users\{}\AppData\Local\Google\Chrome\Application\chrome.exe".format(os.getenv('USERNAME')),
            r"C:\Program Files\Google\Chrome Beta\Application\chrome.exe",
            r"C:\Program Files\Chromium\Application\chrome.exe"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                self.logger.info(f"æ‰¾åˆ°Chrome: {path}")
                return path
        
        self.logger.error("æœªæ‰¾åˆ°Chromeæµè§ˆå™¨")
        return None
        
    def create_driver(self):
        """åˆ›å»ºChrome WebDriverå®ä¾‹"""
        chrome_options = Options()
        
        # æŸ¥æ‰¾Chromeå¯æ‰§è¡Œæ–‡ä»¶
        chrome_path = self.find_chrome_executable()
        if chrome_path:
            chrome_options.binary_location = chrome_path
        
        if self.headless:
            chrome_options.add_argument('--headless')
        
        # ä¼˜åŒ–é€‰é¡¹
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
        chrome_options.add_experimental_option('useAutomationExtension', False)
        
        # è®¾ç½®ç”¨æˆ·ä»£ç†
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        
        try:
            # å°è¯•ä½¿ç”¨ç³»ç»ŸPATHä¸­çš„chromedriver
            driver = webdriver.Chrome(options=chrome_options)
            
            # æ‰§è¡Œåæ£€æµ‹è„šæœ¬
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            return driver
        except Exception as e:
            self.logger.error(f"åˆ›å»ºWebDriverå¤±è´¥: {e}")
            self.logger.info("è¯·ç¡®ä¿å·²å®‰è£…ChromeDriverå¹¶æ·»åŠ åˆ°ç³»ç»ŸPATHä¸­")
            return None
    
    def init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“"""
        self.conn = sqlite3.connect('local_selenium_stories.db', check_same_thread=False)
        self.cursor = self.conn.cursor()
        
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS stories (
                id INTEGER PRIMARY KEY,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                category TEXT,
                word_count INTEGER,
                scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                url TEXT
            )
        ''')
        self.conn.commit()
        self.logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
    
    def wait_for_content_load(self, driver, timeout=20):
        """ç­‰å¾…é¡µé¢å†…å®¹åŠ è½½å®Œæˆ"""
        try:
            # ç­‰å¾…é¡µé¢åŸºæœ¬ç»“æ„åŠ è½½
            WebDriverWait(driver, timeout).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            # ç­‰å¾…æ›´é•¿æ—¶é—´è®©JavaScriptæ‰§è¡Œ
            time.sleep(5)
            
            # å°è¯•ç­‰å¾…ç‰¹å®šå…ƒç´ åŠ è½½
            try:
                # ç­‰å¾…å¯èƒ½çš„å†…å®¹å®¹å™¨
                WebDriverWait(driver, 10).until(
                    EC.any_of(
                        EC.presence_of_element_located((By.CLASS_NAME, "content")),
                        EC.presence_of_element_located((By.CLASS_NAME, "story")),
                        EC.presence_of_element_located((By.TAG_NAME, "article")),
                        EC.presence_of_element_located((By.ID, "content"))
                    )
                )
            except TimeoutException:
                self.logger.debug("æœªæ‰¾åˆ°ç‰¹å®šå†…å®¹å®¹å™¨ï¼Œç»§ç»­å¤„ç†")
            
            return True
        except TimeoutException:
            self.logger.warning(f"é¡µé¢åŠ è½½è¶…æ—¶: {driver.current_url}")
            return False
    
    def extract_story_content(self, driver, story_id):
        """ä»é¡µé¢ä¸­æå–æ•…äº‹å†…å®¹"""
        try:
            # ç­‰å¾…å†…å®¹åŠ è½½
            if not self.wait_for_content_load(driver):
                return None
            
            # è·å–é¡µé¢æºç 
            page_source = driver.page_source
            soup = BeautifulSoup(page_source, 'html.parser')
            
            # ä¿å­˜è°ƒè¯•HTML
            with open(f'local_selenium_debug_{story_id}.html', 'w', encoding='utf-8') as f:
                f.write(page_source)
            
            self.logger.info(f"é¡µé¢æ ‡é¢˜: {driver.title}")
            self.logger.info(f"é¡µé¢URL: {driver.current_url}")
            
            # æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«æ•…äº‹å†…å®¹
            page_text = soup.get_text()
            if '404' in page_text or 'Not Found' in page_text or len(page_text) < 100:
                self.logger.warning(f"æ•…äº‹ {story_id} é¡µé¢å†…å®¹å¼‚å¸¸")
                return None
            
            # å°è¯•å¤šç§æ–¹å¼æå–æ ‡é¢˜
            title = None
            
            # 1. ä»é¡µé¢æ ‡é¢˜æå–
            page_title = driver.title
            if page_title and 'å°æ•…äº‹é“º' not in page_title and len(page_title) < 200:
                title = page_title.strip()
            
            # 2. ä»HTMLå…ƒç´ æå–
            if not title:
                title_selectors = [
                    'h1', 'h2', '.story-title', '.post-title', 
                    '.article-title', '.title', '#title'
                ]
                
                for selector in title_selectors:
                    elements = soup.select(selector)
                    for elem in elements:
                        text = elem.get_text(strip=True)
                        if text and len(text) < 200 and 'å°æ•…äº‹é“º' not in text:
                            title = text
                            break
                    if title:
                        break
            
            # 3. ä½¿ç”¨Seleniumç›´æ¥æŸ¥æ‰¾
            if not title:
                try:
                    title_elements = driver.find_elements(By.XPATH, "//h1 | //h2 | //*[contains(@class, 'title')]")
                    for elem in title_elements:
                        text = elem.text.strip()
                        if text and len(text) < 200 and 'å°æ•…äº‹é“º' not in text:
                            title = text
                            break
                except Exception as e:
                    self.logger.debug(f"Seleniumæ ‡é¢˜æŸ¥æ‰¾å¤±è´¥: {e}")
            
            # æå–å†…å®¹
            content = None
            
            # 1. ä»HTMLå…ƒç´ æå–
            content_selectors = [
                '.story-content', '.post-content', '.article-content',
                '.content', '#content', 'article', '.main-content',
                '.story-body', '.text-content', 'main'
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                for elem in elements:
                    text = elem.get_text(strip=True)
                    if text and len(text) > 100 and 'å°æ•…äº‹é“ºæ±‡é›†äº†å„ç§ç±»å‹' not in text:
                        content = text
                        break
                if content:
                    break
            
            # 2. ä½¿ç”¨Seleniumç›´æ¥æŸ¥æ‰¾
            if not content:
                try:
                    content_elements = driver.find_elements(By.XPATH, 
                        "//div[contains(@class, 'content')] | //div[contains(@class, 'story')] | //article | //main")
                    for elem in content_elements:
                        text = elem.text.strip()
                        if text and len(text) > 100 and 'å°æ•…äº‹é“ºæ±‡é›†äº†å„ç§ç±»å‹' not in text:
                            content = text
                            break
                except Exception as e:
                    self.logger.debug(f"Seleniumå†…å®¹æŸ¥æ‰¾å¤±è´¥: {e}")
            
            # 3. ä»é¡µé¢æ‰€æœ‰æ–‡æœ¬ä¸­æå–
            if not content:
                all_text = soup.get_text()
                paragraphs = [p.strip() for p in all_text.split('\n') if p.strip()]
                long_paragraphs = [p for p in paragraphs if len(p) > 50]
                if long_paragraphs:
                    content = '\n'.join(long_paragraphs[:10])  # å–å‰10ä¸ªé•¿æ®µè½
            
            # éªŒè¯æå–çš„å†…å®¹
            if not title:
                title = f"æ•…äº‹ {story_id}"
            
            if not content or len(content) < 50:
                self.logger.warning(f"æ•…äº‹ {story_id} å†…å®¹å¤ªçŸ­æˆ–ä¸ºç©º")
                return None
            
            # æ¸…ç†å†…å®¹
            content = re.sub(r'\s+', ' ', content).strip()
            
            # æ¨æ–­åˆ†ç±»
            category = self.infer_category(title, content)
            
            # è®¡ç®—å­—æ•°
            word_count = len(content)
            
            story_data = {
                'id': story_id,
                'title': title,
                'content': content,
                'category': category,
                'word_count': word_count,
                'url': driver.current_url
            }
            
            self.logger.info(f"âœ… æˆåŠŸæå–æ•…äº‹ {story_id}: {title[:50]}... (å­—æ•°: {word_count})")
            return story_data
            
        except Exception as e:
            self.logger.error(f"âŒ æå–æ•…äº‹ {story_id} å†…å®¹å¤±è´¥: {e}")
            return None
    
    def infer_category(self, title, content):
        """æ ¹æ®æ ‡é¢˜å’Œå†…å®¹æ¨æ–­æ•…äº‹åˆ†ç±»"""
        categories = {
            'çˆ±æƒ…æ•…äº‹': ['çˆ±æƒ…', 'æ‹çˆ±', 'æƒ…ä¾£', 'ç”·å‹', 'å¥³å‹', 'ç»“å©š', 'å©šå§»', 'æµªæ¼«'],
            'ææ€–æ•…äº‹': ['ææ€–', 'é¬¼', 'çµå¼‚', 'æƒŠæ‚š', 'å¯æ€•', 'é˜´æ£®', 'è¯¡å¼‚'],
            'åŠ±å¿—æ•…äº‹': ['åŠ±å¿—', 'æˆåŠŸ', 'å¥‹æ–—', 'åšæŒ', 'æ¢¦æƒ³', 'åŠªåŠ›', 'æ‹¼æ'],
            'å„¿ç«¥æ•…äº‹': ['å°æœ‹å‹', 'å­©å­', 'ç«¥è¯', 'åŠ¨ç‰©', 'æ£®æ—', 'ç‹å­', 'å…¬ä¸»'],
            'å†å²æ•…äº‹': ['å¤ä»£', 'å†å²', 'æœä»£', 'çš‡å¸', 'å°†å†›', 'æˆ˜äº‰', 'ä¼ è¯´'],
            'ç¥è¯æ•…äº‹': ['ç¥è¯', 'ä»™äºº', 'ç¥ä»™', 'é¾™', 'å‡¤å‡°', 'ä¼ è¯´', 'ç¥'],
            'å¯“è¨€æ•…äº‹': ['å¯“è¨€', 'é“ç†', 'å¯ç¤º', 'æ™ºæ…§', 'å“²ç†'],
            'æ°‘é—´æ•…äº‹': ['æ°‘é—´', 'ä¼ è¯´', 'è€äºº', 'æ‘åº„', 'ä¹¡æ‘'],
            'ç§‘å¹»æ•…äº‹': ['ç§‘å¹»', 'æœªæ¥', 'æœºå™¨äºº', 'å¤ªç©º', 'å¤–æ˜Ÿäºº', 'ç§‘æŠ€'],
            'æ‚¬ç–‘æ•…äº‹': ['æ‚¬ç–‘', 'æ¨ç†', 'ä¾¦æ¢', 'è°œå›¢', 'ç ´æ¡ˆ', 'çº¿ç´¢']
        }
        
        text = (title + ' ' + content).lower()
        
        for category, keywords in categories.items():
            if any(keyword in text for keyword in keywords):
                return category
        
        return 'å…¶ä»–æ•…äº‹'
    
    def crawl_single_story(self, story_id):
        """çˆ¬å–å•ä¸ªæ•…äº‹"""
        driver = None
        try:
            driver = self.create_driver()
            if not driver:
                self.logger.error(f"æ— æ³•åˆ›å»ºWebDriverï¼Œè·³è¿‡æ•…äº‹ {story_id}")
                return None
            
            url = f"{self.base_url}/story/{story_id}"
            self.logger.info(f"ğŸ” æ­£åœ¨çˆ¬å–æ•…äº‹ {story_id}: {url}")
            
            driver.get(url)
            
            story_data = self.extract_story_content(driver, story_id)
            
            if story_data:
                # ä¿å­˜åˆ°æ•°æ®åº“
                self.save_story_to_db(story_data)
                return story_data
            else:
                return None
                
        except Exception as e:
            self.logger.error(f"âŒ çˆ¬å–æ•…äº‹ {story_id} å¤±è´¥: {e}")
            return None
        finally:
            if driver:
                driver.quit()
    
    def save_story_to_db(self, story_data):
        """ä¿å­˜æ•…äº‹åˆ°æ•°æ®åº“"""
        try:
            self.cursor.execute('''
                INSERT OR REPLACE INTO stories 
                (id, title, content, category, word_count, url)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                story_data['id'],
                story_data['title'],
                story_data['content'],
                story_data['category'],
                story_data['word_count'],
                story_data['url']
            ))
            self.conn.commit()
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ•…äº‹åˆ°æ•°æ®åº“å¤±è´¥: {e}")
    
    def crawl_stories(self, start_id=1, end_id=10):
        """æ‰¹é‡çˆ¬å–æ•…äº‹ï¼ˆå•çº¿ç¨‹ï¼‰"""
        self.logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–æ•…äº‹ {start_id} åˆ° {end_id}")
        self.total_count = end_id - start_id + 1
        
        for story_id in range(start_id, end_id + 1):
            try:
                result = self.crawl_single_story(story_id)
                if result:
                    self.stories.append(result)
                    self.success_count += 1
                    self.logger.info(f"âœ… æ•…äº‹ {story_id} çˆ¬å–æˆåŠŸ ({self.success_count}/{self.total_count})")
                else:
                    self.failed_ids.append(story_id)
                    self.logger.warning(f"âŒ æ•…äº‹ {story_id} çˆ¬å–å¤±è´¥")
                    
                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
                time.sleep(2)
                
            except Exception as e:
                self.failed_ids.append(story_id)
                self.logger.error(f"âŒ æ•…äº‹ {story_id} å¤„ç†å¼‚å¸¸: {e}")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        success_rate = (self.success_count / self.total_count) * 100 if self.total_count > 0 else 0
        
        report = {
            'crawl_time': datetime.now().isoformat(),
            'total_stories': self.total_count,
            'successful_stories': self.success_count,
            'failed_stories': len(self.failed_ids),
            'success_rate': f"{success_rate:.2f}%",
            'failed_ids': self.failed_ids,
            'stories_sample': self.stories[:5]  # ä¿å­˜å‰5ä¸ªæ•…äº‹ä½œä¸ºæ ·æœ¬
        }
        
        # ä¿å­˜JSONæŠ¥å‘Š
        with open('local_selenium_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜æ‰€æœ‰æ•…äº‹åˆ°JSON
        with open('local_selenium_stories.json', 'w', encoding='utf-8') as f:
            json.dump(self.stories, f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"""
=== ğŸ‰ çˆ¬å–å®Œæˆ ===
æ€»è®¡æ•…äº‹: {self.total_count}
æˆåŠŸçˆ¬å–: {self.success_count}
å¤±è´¥æ•°é‡: {len(self.failed_ids)}
æˆåŠŸç‡: {success_rate:.2f}%
æŠ¥å‘Šå·²ä¿å­˜åˆ°: local_selenium_report.json
æ•…äº‹æ•°æ®å·²ä¿å­˜åˆ°: local_selenium_stories.json
æ•°æ®åº“: local_selenium_stories.db
        """)
    
    def analyze_content_uniqueness(self):
        """åˆ†æå†…å®¹å”¯ä¸€æ€§"""
        if not self.stories:
            self.logger.warning("æ²¡æœ‰æ•…äº‹æ•°æ®å¯åˆ†æ")
            return
        
        # ç»Ÿè®¡å”¯ä¸€æ ‡é¢˜å’Œå†…å®¹
        unique_titles = set()
        unique_contents = set()
        content_groups = {}
        category_count = {}
        
        for story in self.stories:
            title = story['title']
            content = story['content']
            category = story['category']
            
            unique_titles.add(title)
            unique_contents.add(content)
            
            # æŒ‰å†…å®¹åˆ†ç»„
            if content not in content_groups:
                content_groups[content] = []
            content_groups[content].append(story['id'])
            
            # ç»Ÿè®¡åˆ†ç±»
            category_count[category] = category_count.get(category, 0) + 1
        
        # æ‰¾å‡ºé‡å¤å†…å®¹
        duplicate_groups = {content: ids for content, ids in content_groups.items() if len(ids) > 1}
        
        analysis = {
            'total_stories': len(self.stories),
            'unique_titles': len(unique_titles),
            'unique_contents': len(unique_contents),
            'duplicate_content_groups': len(duplicate_groups),
            'duplicate_story_count': sum(len(ids) for ids in duplicate_groups.values()),
            'category_distribution': category_count,
            'sample_stories': [
                {
                    'id': story['id'],
                    'title': story['title'][:100],
                    'content_preview': story['content'][:200] + '...',
                    'category': story['category'],
                    'word_count': story['word_count']
                }
                for story in self.stories[:3]
            ]
        }
        
        self.logger.info(f"""
=== ğŸ“Š å†…å®¹å”¯ä¸€æ€§åˆ†æ ===
æ€»æ•…äº‹æ•°: {analysis['total_stories']}
å”¯ä¸€æ ‡é¢˜æ•°: {analysis['unique_titles']}
å”¯ä¸€å†…å®¹æ•°: {analysis['unique_contents']}
é‡å¤å†…å®¹ç»„æ•°: {analysis['duplicate_content_groups']}
é‡å¤æ•…äº‹æ€»æ•°: {analysis['duplicate_story_count']}
åˆ†ç±»åˆ†å¸ƒ: {analysis['category_distribution']}
        """)
        
        # ä¿å­˜åˆ†æç»“æœ
        with open('local_selenium_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        return analysis
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if hasattr(self, 'conn'):
            self.conn.close()

def main():
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = LocalSeleniumCrawler(max_workers=1, headless=False)  # éæ— å¤´æ¨¡å¼ä¾¿äºè§‚å¯Ÿ
    
    try:
        # çˆ¬å–å‰10ä¸ªæ•…äº‹è¿›è¡Œæµ‹è¯•
        crawler.crawl_stories(start_id=1, end_id=10)
        
        # åˆ†æå†…å®¹å”¯ä¸€æ€§
        crawler.analyze_content_uniqueness()
        
    except KeyboardInterrupt:
        crawler.logger.info("ç”¨æˆ·ä¸­æ–­çˆ¬å–")
    except Exception as e:
        crawler.logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        crawler.close()

if __name__ == "__main__":
    main()