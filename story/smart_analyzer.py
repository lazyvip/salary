#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ç½‘ç«™åˆ†æå™¨ - åˆ†æ storynook.cn çš„å®é™…ç»“æ„
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from urllib.parse import urljoin
import time

def analyze_website():
    """åˆ†æç½‘ç«™ç»“æ„"""
    base_url = "https://storynook.cn/"
    
    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        print("ğŸ” æ­£åœ¨åˆ†æç½‘ç«™ç»“æ„...")
        response = requests.get(base_url, headers=headers, timeout=15)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        print(f"âœ… æˆåŠŸè·å–ä¸»é¡µï¼ŒçŠ¶æ€ç : {response.status_code}")
        print(f"ğŸ“„ é¡µé¢å¤§å°: {len(response.text)} å­—ç¬¦")
        
        # ä¿å­˜åŸå§‹HTMLç”¨äºåˆ†æ
        with open('website_analysis.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        analysis_result = {
            "url": base_url,
            "status_code": response.status_code,
            "page_size": len(response.text),
            "title": soup.title.string if soup.title else "æ— æ ‡é¢˜",
            "analysis_time": time.strftime("%Y-%m-%d %H:%M:%S"),
            "findings": {}
        }
        
        print(f"ğŸ“ é¡µé¢æ ‡é¢˜: {analysis_result['title']}")
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰JavaScriptæ–‡ä»¶
        scripts = soup.find_all('script', src=True)
        js_files = [urljoin(base_url, script['src']) for script in scripts]
        analysis_result["findings"]["js_files"] = js_files
        print(f"ğŸ”§ å‘ç° {len(js_files)} ä¸ªJavaScriptæ–‡ä»¶")
        
        # 2. æŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
        links = soup.find_all('a', href=True)
        all_links = [urljoin(base_url, link['href']) for link in links]
        analysis_result["findings"]["all_links"] = all_links
        print(f"ğŸ”— å‘ç° {len(all_links)} ä¸ªé“¾æ¥")
        
        # 3. æŸ¥æ‰¾åŒ…å«"story"æˆ–"æ•…äº‹"çš„å…ƒç´ 
        story_elements = []
        
        # æŸ¥æ‰¾onclickäº‹ä»¶
        onclick_elements = soup.find_all(attrs={"onclick": True})
        for elem in onclick_elements:
            onclick = elem.get('onclick', '')
            if 'story' in onclick.lower() or 'æ•…äº‹' in onclick:
                story_elements.append({
                    "type": "onclick",
                    "onclick": onclick,
                    "text": elem.get_text(strip=True)[:100],
                    "tag": elem.name
                })
        
        analysis_result["findings"]["story_elements"] = story_elements
        print(f"ğŸ“š å‘ç° {len(story_elements)} ä¸ªæ•…äº‹ç›¸å…³å…ƒç´ ")
        
        # 4. æŸ¥æ‰¾æ‰€æœ‰è¡¨å•
        forms = soup.find_all('form')
        form_info = []
        for form in forms:
            form_info.append({
                "action": form.get('action', ''),
                "method": form.get('method', 'GET'),
                "inputs": [{"name": inp.get('name', ''), "type": inp.get('type', '')} 
                          for inp in form.find_all('input')]
            })
        analysis_result["findings"]["forms"] = form_info
        print(f"ğŸ“‹ å‘ç° {len(forms)} ä¸ªè¡¨å•")
        
        # 5. æŸ¥æ‰¾å¯èƒ½çš„APIç«¯ç‚¹
        api_patterns = [
            r'/api/[^"\s]+',
            r'\.php[^"\s]*',
            r'\.json[^"\s]*',
            r'/ajax/[^"\s]+',
        ]
        
        potential_apis = []
        page_text = response.text
        for pattern in api_patterns:
            matches = re.findall(pattern, page_text)
            for match in matches:
                full_url = urljoin(base_url, match)
                if full_url not in potential_apis:
                    potential_apis.append(full_url)
        
        analysis_result["findings"]["potential_apis"] = potential_apis
        print(f"ğŸ”Œ å‘ç° {len(potential_apis)} ä¸ªæ½œåœ¨APIç«¯ç‚¹")
        
        # 6. æŸ¥æ‰¾æ‰€æœ‰IDå’ŒClass
        all_ids = [elem.get('id') for elem in soup.find_all(id=True)]
        all_classes = []
        for elem in soup.find_all(class_=True):
            classes = elem.get('class', [])
            if isinstance(classes, list):
                all_classes.extend(classes)
            else:
                all_classes.append(classes)
        
        analysis_result["findings"]["ids"] = list(set(all_ids))
        analysis_result["findings"]["classes"] = list(set(all_classes))
        print(f"ğŸ·ï¸ å‘ç° {len(set(all_ids))} ä¸ªIDï¼Œ{len(set(all_classes))} ä¸ªClass")
        
        # 7. æŸ¥æ‰¾å¯èƒ½çš„æ•°æ®å®¹å™¨
        data_containers = []
        container_selectors = [
            '[data-story]', '[data-id]', '.story', '.tale', 
            '#stories', '#content', '.content', '.list',
            '.card', '.item', '.post'
        ]
        
        for selector in container_selectors:
            elements = soup.select(selector)
            if elements:
                data_containers.append({
                    "selector": selector,
                    "count": len(elements),
                    "sample_text": elements[0].get_text(strip=True)[:100] if elements else ""
                })
        
        analysis_result["findings"]["data_containers"] = data_containers
        print(f"ğŸ“¦ å‘ç° {len(data_containers)} ç§æ•°æ®å®¹å™¨")
        
        # 8. æ£€æŸ¥æ˜¯å¦æœ‰åŠ¨æ€åŠ è½½çš„è¿¹è±¡
        dynamic_indicators = []
        dynamic_keywords = ['ajax', 'fetch', 'xhr', 'loadmore', 'infinite', 'scroll']
        
        for keyword in dynamic_keywords:
            if keyword in page_text.lower():
                dynamic_indicators.append(keyword)
        
        analysis_result["findings"]["dynamic_indicators"] = dynamic_indicators
        print(f"âš¡ å‘ç°åŠ¨æ€åŠ è½½æŒ‡ç¤ºå™¨: {dynamic_indicators}")
        
        # ä¿å­˜åˆ†æç»“æœ
        with open('website_analysis_result.json', 'w', encoding='utf-8') as f:
            json.dump(analysis_result, f, ensure_ascii=False, indent=2)
        
        print("\n" + "="*50)
        print("ğŸ“Š åˆ†æå®Œæˆï¼ç”Ÿæˆæ–‡ä»¶:")
        print("   - website_analysis.html (åŸå§‹HTML)")
        print("   - website_analysis_result.json (åˆ†æç»“æœ)")
        print("="*50)
        
        # è¾“å‡ºå…³é”®å‘ç°
        print("\nğŸ” å…³é”®å‘ç°:")
        if story_elements:
            print(f"   âœ… å‘ç° {len(story_elements)} ä¸ªæ•…äº‹å…ƒç´ ")
            for elem in story_elements[:3]:
                print(f"      - {elem['onclick'][:50]}...")
        
        if potential_apis:
            print(f"   âœ… å‘ç° {len(potential_apis)} ä¸ªæ½œåœ¨API")
            for api in potential_apis[:3]:
                print(f"      - {api}")
        
        if dynamic_indicators:
            print(f"   âš¡ åŠ¨æ€åŠ è½½æŒ‡ç¤ºå™¨: {', '.join(dynamic_indicators)}")
        
        return analysis_result
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        return None

if __name__ == "__main__":
    result = analyze_website()
    if result:
        print("\nâœ… ç½‘ç«™åˆ†æå®Œæˆï¼")
    else:
        print("\nâŒ ç½‘ç«™åˆ†æå¤±è´¥ï¼")