#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºrequestsçš„å¤§è§„æ¨¡çˆ¬è™« - ä¸“é—¨çˆ¬å– storynook.cn çš„æ‰€æœ‰æ•…äº‹
é¿å…ChromeDriverç‰ˆæœ¬é—®é¢˜ï¼Œä½¿ç”¨HTTPè¯·æ±‚ç›´æ¥è·å–æ•°æ®
"""

import time
import json
import re
import sqlite3
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import logging
from urllib.parse import urljoin, urlparse
import random

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('requests_mass_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class RequestsMassCrawler:
    def __init__(self):
        self.base_url = "https://storynook.cn/"
        self.session = requests.Session()
        self.all_stories = {}  # ä½¿ç”¨å­—å…¸å»é‡
        self.story_contents = {}
        self.crawl_stats = {
            "start_time": datetime.now().isoformat(),
            "stories_found": 0,
            "stories_with_content": 0,
            "pages_crawled": 0,
            "errors": 0
        }
        
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
    def get_page_content(self, url, max_retries=3):
        """è·å–é¡µé¢å†…å®¹"""
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=15)
                response.raise_for_status()
                response.encoding = 'utf-8'
                return response.text
            except Exception as e:
                logger.warning(f"è·å–é¡µé¢å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(random.uniform(1, 3))
                else:
                    self.crawl_stats["errors"] += 1
                    return None
        return None
    
    def extract_stories_from_html(self, html_content):
        """ä»HTMLå†…å®¹ä¸­æå–æ•…äº‹ä¿¡æ¯"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            new_stories_count = 0
            
            # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«showStoryçš„onclickå±æ€§
            elements_with_onclick = soup.find_all(attrs={"onclick": re.compile(r'showStory\(\d+\)')})
            
            for element in elements_with_onclick:
                try:
                    onclick = element.get('onclick', '')
                    story_id_match = re.search(r'showStory\((\d+)\)', onclick)
                    if story_id_match:
                        story_id = int(story_id_match.group(1))
                        title = element.get_text(strip=True)
                        
                        if story_id not in self.all_stories and title:
                            self.all_stories[story_id] = {
                                "id": story_id,
                                "title": title,
                                "category_name": "ç¡å‰æ•…äº‹",
                                "extracted_at": datetime.now().isoformat(),
                                "source": "requests_onclick"
                            }
                            new_stories_count += 1
                except Exception as e:
                    continue
            
            # æ–¹æ³•2: ç›´æ¥ä»HTMLæºç ä¸­æ­£åˆ™æå–
            onclick_pattern = r'onclick=["\']showStory\((\d+)\)["\'][^>]*>([^<]+)'
            matches = re.findall(onclick_pattern, html_content)
            
            for story_id, title in matches:
                try:
                    story_id = int(story_id)
                    title = title.strip()
                    if story_id not in self.all_stories and title:
                        self.all_stories[story_id] = {
                            "id": story_id,
                            "title": title,
                            "category_name": "ç¡å‰æ•…äº‹",
                            "extracted_at": datetime.now().isoformat(),
                            "source": "requests_regex"
                        }
                        new_stories_count += 1
                except:
                    continue
            
            # æ–¹æ³•3: æŸ¥æ‰¾å¯èƒ½çš„æ•…äº‹é“¾æ¥
            story_links = soup.find_all('a', href=re.compile(r'story|tale'))
            for link in story_links:
                try:
                    href = link.get('href', '')
                    title = link.get_text(strip=True)
                    
                    # å°è¯•ä»hrefä¸­æå–ID
                    id_match = re.search(r'(\d+)', href)
                    if id_match and title:
                        story_id = int(id_match.group(1))
                        if story_id not in self.all_stories:
                            self.all_stories[story_id] = {
                                "id": story_id,
                                "title": title,
                                "category_name": "ç¡å‰æ•…äº‹",
                                "extracted_at": datetime.now().isoformat(),
                                "source": "requests_link"
                            }
                            new_stories_count += 1
                except:
                    continue
            
            logger.info(f"ä»HTMLæå–åˆ° {new_stories_count} ä¸ªæ–°æ•…äº‹ï¼Œæ€»è®¡ {len(self.all_stories)} ä¸ª")
            return new_stories_count
            
        except Exception as e:
            logger.error(f"HTMLè§£æå¤±è´¥: {e}")
            return 0
    
    def try_get_story_content_api(self, story_id):
        """å°è¯•é€šè¿‡APIè·å–æ•…äº‹å†…å®¹"""
        api_urls = [
            f"{self.base_url}api/story/{story_id}",
            f"{self.base_url}story/{story_id}",
            f"{self.base_url}get_story.php?id={story_id}",
            f"{self.base_url}story.php?id={story_id}",
        ]
        
        for api_url in api_urls:
            try:
                response = self.session.get(api_url, timeout=10)
                if response.status_code == 200:
                    # å°è¯•JSONè§£æ
                    try:
                        data = response.json()
                        if isinstance(data, dict) and 'content' in data:
                            return data['content']
                        elif isinstance(data, dict) and 'story' in data:
                            return data['story']
                    except:
                        pass
                    
                    # å°è¯•HTMLè§£æ
                    if len(response.text) > 100:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # æŸ¥æ‰¾å¯èƒ½çš„å†…å®¹å®¹å™¨
                        content_selectors = [
                            '.story-content',
                            '.content',
                            '#content',
                            '.story-text',
                            '.text',
                            'p'
                        ]
                        
                        for selector in content_selectors:
                            elements = soup.select(selector)
                            if elements:
                                content = ' '.join([elem.get_text(strip=True) for elem in elements])
                                if len(content) > 50:
                                    return content
                        
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šå®¹å™¨ï¼Œå°è¯•è·å–æ‰€æœ‰æ–‡æœ¬
                        all_text = soup.get_text(strip=True)
                        if len(all_text) > 100:
                            return all_text
                            
            except Exception as e:
                continue
        
        return None
    
    def crawl_main_page_variations(self):
        """çˆ¬å–ä¸»é¡µçš„å„ç§å˜ä½“"""
        urls_to_try = [
            self.base_url,
            f"{self.base_url}index.html",
            f"{self.base_url}index.php",
            f"{self.base_url}stories",
            f"{self.base_url}stories.html",
            f"{self.base_url}list",
            f"{self.base_url}list.html",
        ]
        
        total_new_stories = 0
        
        for url in urls_to_try:
            try:
                logger.info(f"æ­£åœ¨çˆ¬å–: {url}")
                html_content = self.get_page_content(url)
                
                if html_content:
                    new_stories = self.extract_stories_from_html(html_content)
                    total_new_stories += new_stories
                    self.crawl_stats["pages_crawled"] += 1
                    
                    # éšæœºå»¶è¿Ÿï¼Œé¿å…è¢«å°
                    time.sleep(random.uniform(1, 3))
                
            except Exception as e:
                logger.error(f"çˆ¬å– {url} å¤±è´¥: {e}")
                continue
        
        return total_new_stories
    
    def try_pagination_discovery(self):
        """å°è¯•å‘ç°åˆ†é¡µ"""
        try:
            html_content = self.get_page_content(self.base_url)
            if not html_content:
                return []
            
            soup = BeautifulSoup(html_content, 'html.parser')
            pagination_urls = []
            
            # æŸ¥æ‰¾åˆ†é¡µé“¾æ¥
            pagination_patterns = [
                r'page=(\d+)',
                r'p=(\d+)',
                r'/(\d+)\.html',
                r'/page/(\d+)',
            ]
            
            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link.get('href', '')
                for pattern in pagination_patterns:
                    matches = re.findall(pattern, href)
                    if matches:
                        for page_num in matches:
                            try:
                                page_num = int(page_num)
                                if 1 <= page_num <= 50:  # é™åˆ¶é¡µæ•°èŒƒå›´
                                    full_url = urljoin(self.base_url, href)
                                    if full_url not in pagination_urls:
                                        pagination_urls.append(full_url)
                            except:
                                continue
            
            # å°è¯•æ„é€ å¯èƒ½çš„åˆ†é¡µURL
            for page in range(2, 21):  # å°è¯•2-20é¡µ
                possible_urls = [
                    f"{self.base_url}?page={page}",
                    f"{self.base_url}?p={page}",
                    f"{self.base_url}page/{page}",
                    f"{self.base_url}{page}.html",
                    f"{self.base_url}list?page={page}",
                ]
                
                for url in possible_urls:
                    if url not in pagination_urls:
                        pagination_urls.append(url)
            
            logger.info(f"å‘ç° {len(pagination_urls)} ä¸ªå¯èƒ½çš„åˆ†é¡µURL")
            return pagination_urls[:30]  # é™åˆ¶æ•°é‡
            
        except Exception as e:
            logger.error(f"åˆ†é¡µå‘ç°å¤±è´¥: {e}")
            return []
    
    def crawl_pagination_pages(self, pagination_urls):
        """çˆ¬å–åˆ†é¡µé¡µé¢"""
        total_new_stories = 0
        
        for i, url in enumerate(pagination_urls):
            try:
                logger.info(f"æ­£åœ¨çˆ¬å–åˆ†é¡µ {i+1}/{len(pagination_urls)}: {url}")
                html_content = self.get_page_content(url)
                
                if html_content:
                    new_stories = self.extract_stories_from_html(html_content)
                    total_new_stories += new_stories
                    self.crawl_stats["pages_crawled"] += 1
                    
                    # å¦‚æœè¿ç»­å‡ é¡µéƒ½æ²¡æœ‰æ–°æ•…äº‹ï¼Œå¯èƒ½å·²ç»åˆ°åº•äº†
                    if new_stories == 0 and i > 5:
                        consecutive_empty = 0
                        for j in range(max(0, i-3), i):
                            if j < len(pagination_urls):
                                consecutive_empty += 1
                        if consecutive_empty >= 3:
                            logger.info("è¿ç»­å¤šé¡µæ— æ–°æ•…äº‹ï¼Œåœæ­¢åˆ†é¡µçˆ¬å–")
                            break
                    
                    # éšæœºå»¶è¿Ÿ
                    time.sleep(random.uniform(2, 5))
                
            except Exception as e:
                logger.error(f"çˆ¬å–åˆ†é¡µ {url} å¤±è´¥: {e}")
                continue
        
        return total_new_stories
    
    def get_stories_content_batch(self, max_stories=100):
        """æ‰¹é‡è·å–æ•…äº‹å†…å®¹"""
        try:
            story_ids = list(self.all_stories.keys())[:max_stories]
            logger.info(f"å¼€å§‹è·å– {len(story_ids)} ä¸ªæ•…äº‹çš„å†…å®¹...")
            
            success_count = 0
            
            for i, story_id in enumerate(story_ids):
                try:
                    content = self.try_get_story_content_api(story_id)
                    
                    if content and len(content) > 50:
                        self.story_contents[story_id] = {
                            "id": story_id,
                            "content": content,
                            "length": len(content),
                            "extracted_at": datetime.now().isoformat()
                        }
                        
                        # æ›´æ–°æ•…äº‹ä¿¡æ¯
                        if story_id in self.all_stories:
                            self.all_stories[story_id]["content_length"] = len(content)
                            self.all_stories[story_id]["has_content"] = True
                        
                        success_count += 1
                    
                    if (i + 1) % 10 == 0:
                        logger.info(f"å·²å¤„ç† {i + 1}/{len(story_ids)} ä¸ªæ•…äº‹ï¼ŒæˆåŠŸ: {success_count}")
                    
                    # éšæœºå»¶è¿Ÿ
                    time.sleep(random.uniform(0.5, 2))
                    
                except Exception as e:
                    logger.warning(f"è·å–æ•…äº‹ {story_id} å†…å®¹å¤±è´¥: {e}")
                    continue
            
            logger.info(f"å†…å®¹è·å–å®Œæˆï¼ŒæˆåŠŸè·å– {success_count} ä¸ªæ•…äº‹å†…å®¹")
            self.crawl_stats["stories_with_content"] = success_count
            return success_count
            
        except Exception as e:
            logger.error(f"æ‰¹é‡è·å–æ•…äº‹å†…å®¹å¤±è´¥: {e}")
            return 0
    
    def save_all_data(self):
        """ä¿å­˜æ‰€æœ‰æ•°æ®"""
        try:
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            self.crawl_stats["end_time"] = datetime.now().isoformat()
            self.crawl_stats["stories_found"] = len(self.all_stories)
            
            # ä¿å­˜æ•…äº‹åˆ—è¡¨
            stories_list = list(self.all_stories.values())
            with open('requests_crawled_stories.json', 'w', encoding='utf-8') as f:
                json.dump(stories_list, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜æ•…äº‹å†…å®¹
            if self.story_contents:
                contents_list = list(self.story_contents.values())
                with open('requests_story_contents.json', 'w', encoding='utf-8') as f:
                    json.dump(contents_list, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜åˆ°SQLiteæ•°æ®åº“
            self.save_to_database()
            
            # ä¿å­˜çˆ¬å–æŠ¥å‘Š
            with open('requests_crawl_report.json', 'w', encoding='utf-8') as f:
                json.dump(self.crawl_stats, f, ensure_ascii=False, indent=2)
            
            logger.info("æ‰€æœ‰æ•°æ®ä¿å­˜æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}")
            return False
    
    def save_to_database(self):
        """ä¿å­˜åˆ°SQLiteæ•°æ®åº“"""
        try:
            conn = sqlite3.connect('requests_crawled_stories.db')
            cursor = conn.cursor()
            
            # åˆ›å»ºæ•…äº‹è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS stories (
                    id INTEGER PRIMARY KEY,
                    title TEXT NOT NULL,
                    category_name TEXT,
                    content_length INTEGER DEFAULT 0,
                    has_content BOOLEAN DEFAULT FALSE,
                    extracted_at TEXT,
                    source TEXT
                )
            ''')
            
            # åˆ›å»ºæ•…äº‹å†…å®¹è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS story_contents (
                    id INTEGER PRIMARY KEY,
                    content TEXT NOT NULL,
                    length INTEGER,
                    extracted_at TEXT
                )
            ''')
            
            # æ’å…¥æ•…äº‹æ•°æ®
            for story in self.all_stories.values():
                cursor.execute('''
                    INSERT OR REPLACE INTO stories 
                    (id, title, category_name, content_length, has_content, extracted_at, source)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    story["id"],
                    story["title"],
                    story.get("category_name", "ç¡å‰æ•…äº‹"),
                    story.get("content_length", 0),
                    story.get("has_content", False),
                    story["extracted_at"],
                    story.get("source", "unknown")
                ))
            
            # æ’å…¥æ•…äº‹å†…å®¹
            for content in self.story_contents.values():
                cursor.execute('''
                    INSERT OR REPLACE INTO story_contents 
                    (id, content, length, extracted_at)
                    VALUES (?, ?, ?, ?)
                ''', (
                    content["id"],
                    content["content"],
                    content["length"],
                    content["extracted_at"]
                ))
            
            conn.commit()
            conn.close()
            
            logger.info("æ•°æ®åº“ä¿å­˜æˆåŠŸ")
            return True
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def run_mass_crawl(self):
        """è¿è¡Œå¤§è§„æ¨¡çˆ¬å–"""
        try:
            logger.info("å¼€å§‹åŸºäºrequestsçš„å¤§è§„æ¨¡æ•…äº‹çˆ¬å–...")
            
            # 1. çˆ¬å–ä¸»é¡µåŠå…¶å˜ä½“
            logger.info("æ­¥éª¤1: çˆ¬å–ä¸»é¡µåŠå˜ä½“...")
            main_stories = self.crawl_main_page_variations()
            logger.info(f"ä¸»é¡µçˆ¬å–å®Œæˆï¼Œå‘ç° {main_stories} ä¸ªæ–°æ•…äº‹")
            
            # 2. å‘ç°å¹¶çˆ¬å–åˆ†é¡µ
            logger.info("æ­¥éª¤2: å‘ç°åˆ†é¡µ...")
            pagination_urls = self.try_pagination_discovery()
            
            if pagination_urls:
                logger.info("æ­¥éª¤3: çˆ¬å–åˆ†é¡µ...")
                pagination_stories = self.crawl_pagination_pages(pagination_urls)
                logger.info(f"åˆ†é¡µçˆ¬å–å®Œæˆï¼Œå‘ç° {pagination_stories} ä¸ªæ–°æ•…äº‹")
            
            # 3. è·å–æ•…äº‹å†…å®¹
            if self.all_stories:
                logger.info("æ­¥éª¤4: è·å–æ•…äº‹å†…å®¹...")
                self.get_stories_content_batch(min(100, len(self.all_stories)))
            
            # 4. ä¿å­˜æ‰€æœ‰æ•°æ®
            logger.info("æ­¥éª¤5: ä¿å­˜æ•°æ®...")
            self.save_all_data()
            
            logger.info("å¤§è§„æ¨¡çˆ¬å–å®Œæˆï¼")
            logger.info(f"æ€»å…±å‘ç° {len(self.all_stories)} ä¸ªæ•…äº‹")
            logger.info(f"è·å–åˆ° {len(self.story_contents)} ä¸ªæ•…äº‹çš„å®Œæ•´å†…å®¹")
            logger.info(f"çˆ¬å–äº† {self.crawl_stats['pages_crawled']} ä¸ªé¡µé¢")
            
            return True
            
        except Exception as e:
            logger.error(f"å¤§è§„æ¨¡çˆ¬å–å¤±è´¥: {e}")
            return False

def main():
    crawler = RequestsMassCrawler()
    success = crawler.run_mass_crawl()
    
    if success:
        print("âœ… åŸºäºrequestsçš„å¤§è§„æ¨¡çˆ¬å–å®Œæˆï¼")
        print("ğŸ“Š çˆ¬å–ç»“æœ:")
        print(f"   - å‘ç°æ•…äº‹: {len(crawler.all_stories)} ä¸ª")
        print(f"   - è·å–å†…å®¹: {len(crawler.story_contents)} ä¸ª")
        print(f"   - çˆ¬å–é¡µé¢: {crawler.crawl_stats['pages_crawled']} ä¸ª")
        print("ğŸ“ ç”Ÿæˆæ–‡ä»¶:")
        print("   - requests_crawled_stories.json (æ‰€æœ‰æ•…äº‹åˆ—è¡¨)")
        print("   - requests_story_contents.json (æ•…äº‹å†…å®¹)")
        print("   - requests_crawled_stories.db (SQLiteæ•°æ®åº“)")
        print("   - requests_crawl_report.json (çˆ¬å–æŠ¥å‘Š)")
    else:
        print("âŒ å¤§è§„æ¨¡çˆ¬å–å¤±è´¥ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶")

if __name__ == "__main__":
    main()