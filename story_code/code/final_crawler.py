#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆæ•…äº‹çˆ¬è™« - ä½¿ç”¨æ­£ç¡®çš„URLæ¨¡å¼
ä¸“é—¨é’ˆå¯¹ https://storynook.cn/?id={æ•°å­—} æ ¼å¼ä¼˜åŒ–
"""

import requests
import json
import time
import sqlite3
import os
import logging
from bs4 import BeautifulSoup
import random
from datetime import datetime
import re
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('final_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FinalStoryCrawler:
    def __init__(self, max_workers=5):
        self.base_url = "https://storynook.cn"
        self.session = requests.Session()
        self.max_workers = max_workers
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # åˆ›å»ºæ•°æ®ç›®å½•
        self.data_dir = "final_stories"
        os.makedirs(self.data_dir, exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®åº“
        self.db_path = os.path.join(self.data_dir, "all_stories.db")
        self.init_database()
        
        # çº¿ç¨‹é”
        self.db_lock = threading.Lock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_attempted': 0,
            'successful_downloads': 0,
            'failed_downloads': 0,
            'empty_stories': 0,
            'categories_found': set()
        }
        
        self.story_data = {}
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS stories (
                id INTEGER PRIMARY KEY,
                story_id INTEGER UNIQUE,
                title TEXT NOT NULL,
                content TEXT,
                category TEXT,
                url TEXT,
                word_count INTEGER,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS categories (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT UNIQUE,
                story_count INTEGER DEFAULT 0
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_story_id ON stories(story_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_category ON stories(category)')
        
        conn.commit()
        conn.close()
        logger.info(f"æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
    
    def make_request(self, url, **kwargs):
        """å‘é€HTTPè¯·æ±‚ï¼Œå¸¦é‡è¯•æœºåˆ¶"""
        max_retries = 3
        for attempt in range(max_retries):
            try:
                # éšæœºå»¶è¿Ÿ
                time.sleep(random.uniform(0.5, 1.5))
                
                response = self.session.get(url, timeout=10, **kwargs)
                response.raise_for_status()
                return response
                
            except Exception as e:
                if attempt == max_retries - 1:
                    logger.warning(f"è¯·æ±‚å¤±è´¥: {url} - {e}")
                    return None
                time.sleep(random.uniform(1, 3))
        
        return None
    
    def extract_story_from_html(self, html_content, story_id, url):
        """ä»HTMLä¸­æå–æ•…äº‹ä¿¡æ¯"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # ç§»é™¤è„šæœ¬å’Œæ ·å¼
        for script in soup(["script", "style"]):
            script.decompose()
        
        # æå–æ ‡é¢˜ - å°è¯•å¤šç§é€‰æ‹©å™¨
        title = None
        title_selectors = [
            'h1', 'h2', '.title', '.story-title', 
            '.content-title', '.article-title', '.post-title',
            '[class*="title"]', '[id*="title"]'
        ]
        
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text().strip()
                if title and len(title) > 2 and title != "å°æ•…äº‹é“º":
                    break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ä¸“é—¨çš„æ ‡é¢˜ï¼Œä»é¡µé¢å†…å®¹ä¸­æå–
        if not title or title == "å°æ•…äº‹é“º":
            # æŸ¥æ‰¾å¯èƒ½çš„æ ‡é¢˜æ¨¡å¼
            text_content = soup.get_text()
            lines = [line.strip() for line in text_content.split('\n') if line.strip()]
            
            for line in lines:
                if (len(line) > 3 and len(line) < 50 and 
                    not any(skip in line for skip in ['å°æ•…äº‹é“º', 'é¦–é¡µ', 'å¯¼èˆª', 'èœå•', 'ç‰ˆæƒ'])):
                    title = line
                    break
        
        # æå–å†…å®¹
        content = None
        content_selectors = [
            '.content', '.story-content', '.article-content',
            '.post-content', '.text', '.story-text',
            'main', '.main-content', '#content',
            '[class*="content"]', '[class*="story"]',
            '[class*="article"]', '[class*="text"]'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text().strip()
                if content and len(content) > 50:
                    break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ä¸“é—¨çš„å†…å®¹åŒºåŸŸï¼Œæå–æ‰€æœ‰æ®µè½
        if not content:
            paragraphs = soup.find_all('p')
            if paragraphs:
                content = '\n'.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
        
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å†…å®¹ï¼Œæå–ä¸»è¦æ–‡æœ¬
        if not content:
            # ç§»é™¤å¯¼èˆªã€èœå•ç­‰å…ƒç´ 
            for elem in soup(['nav', 'header', 'footer', 'aside', '.nav', '.menu', '.header', '.footer']):
                elem.decompose()
            
            content = soup.get_text()
            # æ¸…ç†å†…å®¹
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            content = '\n'.join(lines)
        
        # æ¨æ–­åˆ†ç±»
        category = self.infer_category(title, content)
        
        # éªŒè¯æ•°æ®è´¨é‡
        if not title:
            title = f"æ•…äº‹ {story_id}"
        
        if not content or len(content) < 20:
            return None
        
        # æ¸…ç†å†…å®¹
        content = self.clean_content(content)
        
        return {
            'story_id': story_id,
            'title': title,
            'content': content,
            'category': category,
            'url': url,
            'word_count': len(content),
            'created_at': datetime.now().isoformat()
        }
    
    def infer_category(self, title, content):
        """æ ¹æ®æ ‡é¢˜å’Œå†…å®¹æ¨æ–­åˆ†ç±»"""
        text_to_analyze = f"{title} {content[:200]}".lower()
        
        category_keywords = {
            'ç¡å‰æ•…äº‹': ['ç¡å‰', 'æ™šå®‰', 'æ¢¦', 'åºŠ', 'å¤œæ™š', 'æœˆäº®', 'æ˜Ÿæ˜Ÿ'],
            'å„¿ç«¥æ•…äº‹': ['å°æœ‹å‹', 'å­©å­', 'å®å®', 'å¹¼å„¿', 'å°å­¦', 'ç«¥å¹´'],
            'ç«¥è¯æ•…äº‹': ['å…¬ä¸»', 'ç‹å­', 'é­”æ³•', 'ä»™å¥³', 'å·«å¸ˆ', 'åŸå ¡', 'æ£®æ—'],
            'å¯“è¨€æ•…äº‹': ['å¯“è¨€', 'é“ç†', 'æ•™è®­', 'æ™ºæ…§', 'å“²ç†'],
            'å†å²æ•…äº‹': ['å¤ä»£', 'å†å²', 'æœä»£', 'çš‡å¸', 'å°†å†›', 'æˆ˜äº‰'],
            'ç¥è¯æ•…äº‹': ['ç¥è¯', 'ç¥ä»™', 'å¤©ç¥', 'é¾™', 'å‡¤å‡°', 'ä¼ è¯´'],
            'æ°‘é—´æ•…äº‹': ['æ°‘é—´', 'ä¼ è¯´', 'è€ç™¾å§“', 'æ‘åº„', 'ä¹¡æ‘'],
            'çˆ±æƒ…æ•…äº‹': ['çˆ±æƒ…', 'æ‹äºº', 'æƒ…ä¾£', 'ç»“å©š', 'å©šç¤¼', 'æµªæ¼«'],
            'åŠ±å¿—æ•…äº‹': ['åŠ±å¿—', 'å¥‹æ–—', 'æˆåŠŸ', 'åšæŒ', 'åŠªåŠ›', 'æ¢¦æƒ³'],
            'ææ€–æ•…äº‹': ['ææ€–', 'é¬¼', 'å¹½çµ', 'å®³æ€•', 'é»‘æš—', 'è¯¡å¼‚']
        }
        
        for category, keywords in category_keywords.items():
            if any(keyword in text_to_analyze for keyword in keywords):
                return category
        
        return 'å…¶ä»–æ•…äº‹'
    
    def clean_content(self, content):
        """æ¸…ç†å†…å®¹"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½
        content = re.sub(r'\s+', ' ', content)
        content = re.sub(r'\n\s*\n', '\n\n', content)
        
        # ç§»é™¤å¸¸è§çš„æ— ç”¨æ–‡æœ¬
        unwanted_patterns = [
            r'å°æ•…äº‹é“º.*?é¦–é¡µ',
            r'ç‰ˆæƒæ‰€æœ‰.*',
            r'Copyright.*',
            r'All rights reserved.*',
            r'ç½‘ç«™å¯¼èˆª.*',
            r'è¿”å›é¦–é¡µ.*'
        ]
        
        for pattern in unwanted_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE)
        
        return content.strip()
    
    def crawl_story(self, story_id):
        """çˆ¬å–å•ä¸ªæ•…äº‹"""
        url = f"{self.base_url}/?id={story_id}"
        
        try:
            response = self.make_request(url)
            if not response:
                self.stats['failed_downloads'] += 1
                return None
            
            story_data = self.extract_story_from_html(response.text, story_id, url)
            
            if story_data:
                self.save_story_to_db(story_data)
                self.stats['successful_downloads'] += 1
                self.stats['categories_found'].add(story_data['category'])
                
                logger.info(f"âœ… æ•…äº‹ {story_id}: {story_data['title'][:30]}... ({story_data['category']})")
                return story_data
            else:
                self.stats['empty_stories'] += 1
                logger.debug(f"âŒ æ•…äº‹ {story_id}: å†…å®¹ä¸ºç©ºæˆ–æ— æ•ˆ")
                return None
                
        except Exception as e:
            self.stats['failed_downloads'] += 1
            logger.warning(f"âŒ æ•…äº‹ {story_id} çˆ¬å–å¤±è´¥: {e}")
            return None
    
    def save_story_to_db(self, story):
        """ä¿å­˜æ•…äº‹åˆ°æ•°æ®åº“"""
        with self.db_lock:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            try:
                cursor.execute('''
                    INSERT OR REPLACE INTO stories 
                    (story_id, title, content, category, url, word_count)
                    VALUES (?, ?, ?, ?, ?, ?)
                ''', (
                    story['story_id'],
                    story['title'],
                    story['content'],
                    story['category'],
                    story['url'],
                    story['word_count']
                ))
                
                # æ›´æ–°åˆ†ç±»ç»Ÿè®¡
                cursor.execute('''
                    INSERT OR REPLACE INTO categories (name, story_count)
                    VALUES (?, (
                        SELECT COUNT(*) FROM stories WHERE category = ?
                    ))
                ''', (story['category'], story['category']))
                
                conn.commit()
                
            except Exception as e:
                logger.error(f"ä¿å­˜æ•…äº‹åˆ°æ•°æ®åº“å¤±è´¥: {e}")
            finally:
                conn.close()
    
    def crawl_range(self, start_id, end_id):
        """çˆ¬å–æŒ‡å®šèŒƒå›´çš„æ•…äº‹"""
        logger.info(f"å¼€å§‹çˆ¬å–æ•…äº‹ {start_id} åˆ° {end_id}")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_id = {
                executor.submit(self.crawl_story, story_id): story_id 
                for story_id in range(start_id, end_id + 1)
            }
            
            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for future in as_completed(future_to_id):
                story_id = future_to_id[future]
                self.stats['total_attempted'] += 1
                
                try:
                    story_data = future.result()
                    if story_data:
                        # æŒ‰åˆ†ç±»ç»„ç»‡æ•°æ®
                        category = story_data['category']
                        if category not in self.story_data:
                            self.story_data[category] = []
                        self.story_data[category].append(story_data)
                
                except Exception as e:
                    logger.error(f"å¤„ç†æ•…äº‹ {story_id} ç»“æœæ—¶å‡ºé”™: {e}")
                
                # æ¯100ä¸ªæ•…äº‹æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if self.stats['total_attempted'] % 100 == 0:
                    self.show_progress()
    
    def show_progress(self):
        """æ˜¾ç¤ºè¿›åº¦"""
        total = self.stats['total_attempted']
        success = self.stats['successful_downloads']
        failed = self.stats['failed_downloads']
        empty = self.stats['empty_stories']
        
        success_rate = (success / total * 100) if total > 0 else 0
        
        logger.info(f"ğŸ“Š è¿›åº¦: {total} ä¸ªå·²å°è¯•, {success} ä¸ªæˆåŠŸ ({success_rate:.1f}%), {failed} ä¸ªå¤±è´¥, {empty} ä¸ªç©ºå†…å®¹")
        logger.info(f"ğŸ“š å·²å‘ç° {len(self.stats['categories_found'])} ä¸ªåˆ†ç±»: {', '.join(list(self.stats['categories_found'])[:5])}...")
    
    def save_to_json(self):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        if self.story_data:
            json_file = os.path.join(self.data_dir, "all_stories.json")
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.story_data, f, ensure_ascii=False, indent=2)
            logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {json_file}")
            return json_file
        return None
    
    def get_statistics(self):
        """è·å–æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ€»æ•…äº‹æ•°
        cursor.execute("SELECT COUNT(*) FROM stories")
        total_stories = cursor.fetchone()[0]
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        cursor.execute("SELECT category, COUNT(*) FROM stories GROUP BY category ORDER BY COUNT(*) DESC")
        category_stats = cursor.fetchall()
        
        # å¹³å‡å­—æ•°
        cursor.execute("SELECT AVG(word_count) FROM stories WHERE word_count > 0")
        avg_words = cursor.fetchone()[0] or 0
        
        conn.close()
        
        return {
            'total_stories': total_stories,
            'categories': dict(category_stats),
            'average_words': int(avg_words),
            'attempted': self.stats['total_attempted'],
            'successful': self.stats['successful_downloads'],
            'failed': self.stats['failed_downloads'],
            'empty': self.stats['empty_stories']
        }
    
    def run(self, max_story_id=2000):
        """è¿è¡Œå®Œæ•´çš„çˆ¬å–æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹æœ€ç»ˆæ•…äº‹çˆ¬å–ä»»åŠ¡...")
        logger.info(f"ç›®æ ‡ç½‘ç«™: {self.base_url}")
        logger.info(f"URLæ¨¡å¼: {self.base_url}/?id={{æ•°å­—}}")
        logger.info(f"æœ€å¤§æ•…äº‹ID: {max_story_id}")
        logger.info(f"å¹¶å‘çº¿ç¨‹æ•°: {self.max_workers}")
        
        start_time = datetime.now()
        
        # åˆ†æ‰¹çˆ¬å–ï¼Œé¿å…ä¸€æ¬¡æ€§åˆ›å»ºå¤ªå¤šçº¿ç¨‹
        batch_size = 500
        for start_id in range(1, max_story_id + 1, batch_size):
            end_id = min(start_id + batch_size - 1, max_story_id)
            
            logger.info(f"ğŸ”„ çˆ¬å–æ‰¹æ¬¡: {start_id} - {end_id}")
            self.crawl_range(start_id, end_id)
            
            # æ‰¹æ¬¡é—´ä¼‘æ¯
            time.sleep(2)
        
        # ä¿å­˜æ•°æ®
        json_file = self.save_to_json()
        
        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡
        stats = self.get_statistics()
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info("=" * 60)
        logger.info("ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆï¼")
        logger.info("=" * 60)
        logger.info(f"â±ï¸  æ€»è€—æ—¶: {duration}")
        logger.info(f"ğŸ“Š å°è¯•çˆ¬å–: {stats['attempted']} ä¸ªæ•…äº‹")
        logger.info(f"âœ… æˆåŠŸè·å–: {stats['total_stories']} ä¸ªæ•…äº‹")
        logger.info(f"âŒ å¤±è´¥: {stats['failed']} ä¸ª")
        logger.info(f"ğŸ“ å¹³å‡å­—æ•°: {stats['average_words']} å­—")
        logger.info(f"ğŸ“š åˆ†ç±»æ•°é‡: {len(stats['categories'])} ä¸ª")
        
        logger.info("\nğŸ“‹ åˆ†ç±»ç»Ÿè®¡:")
        for category, count in list(stats['categories'].items())[:10]:
            logger.info(f"  {category}: {count} ä¸ªæ•…äº‹")
        
        logger.info(f"\nğŸ’¾ æ•°æ®åº“æ–‡ä»¶: {self.db_path}")
        if json_file:
            logger.info(f"ğŸ“„ JSONæ–‡ä»¶: {json_file}")
        
        return stats

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨æœ€ç»ˆæ•…äº‹çˆ¬è™«...")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = FinalStoryCrawler(max_workers=8)  # å¢åŠ å¹¶å‘æ•°
    
    try:
        # è¿è¡Œçˆ¬å–ä»»åŠ¡ï¼Œå°è¯•çˆ¬å–å‰2000ä¸ªæ•…äº‹
        stats = crawler.run(max_story_id=2000)
        
        print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼æˆåŠŸè·å–äº† {stats['total_stories']} ä¸ªæ•…äº‹ï¼")
        print(f"ğŸ“Š æˆåŠŸç‡: {stats['total_stories']/stats['attempted']*100:.1f}%")
        
        return stats
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­çˆ¬å–")
        return crawler.get_statistics()
    except Exception as e:
        logger.error(f"çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()