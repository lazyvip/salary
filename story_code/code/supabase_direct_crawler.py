#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Supabaseç›´æ¥APIçˆ¬è™« - ç›´æ¥ä»Supabaseæ•°æ®åº“è·å–æ‰€æœ‰æ•…äº‹
"""

import requests
import json
import sqlite3
import time
from datetime import datetime

class SupabaseDirectCrawler:
    def __init__(self):
        # Supabaseé…ç½®ï¼ˆä»JSæ–‡ä»¶ä¸­æå–ï¼‰
        self.supabase_url = "https://cxtvmolpayeplvdxcjvf.supabase.co"
        self.supabase_key = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImN4dHZtb2xwYXllcGx2ZHhjanZmIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDU1NTE0NTgsImV4cCI6MjA2MTEyNzQ1OH0.LIBg-OUlZHE6jtct1hRBFn1uOw6upqyDOCK-qBqaXic"
        
        # APIç«¯ç‚¹
        self.rest_url = f"{self.supabase_url}/rest/v1"
        
        # è¯·æ±‚å¤´
        self.headers = {
            'apikey': self.supabase_key,
            'Authorization': f'Bearer {self.supabase_key}',
            'Content-Type': 'application/json',
            'Prefer': 'return=representation'
        }
        
        # æ•°æ®å­˜å‚¨
        self.stories = []
        self.story_contents = []
        self.categories = []
        
        print("ğŸš€ Supabaseç›´æ¥APIçˆ¬è™«åˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“¡ APIç«¯ç‚¹: {self.rest_url}")

    def fetch_table_data(self, table_name, limit=1000, offset=0, order_by=None):
        """ä»Supabaseè¡¨ä¸­è·å–æ•°æ®"""
        try:
            url = f"{self.rest_url}/{table_name}"
            params = {
                'limit': limit,
                'offset': offset
            }
            
            if order_by:
                params['order'] = order_by
            
            print(f"ğŸ” è·å–è¡¨ {table_name} æ•°æ® (limit={limit}, offset={offset})")
            
            response = requests.get(url, headers=self.headers, params=params, timeout=30)
            response.raise_for_status()
            
            data = response.json()
            print(f"âœ… æˆåŠŸè·å– {len(data)} æ¡è®°å½•")
            
            return data
            
        except Exception as e:
            print(f"âŒ è·å–è¡¨ {table_name} å¤±è´¥: {e}")
            return []

    def get_all_table_data(self, table_name, order_by=None):
        """è·å–è¡¨çš„æ‰€æœ‰æ•°æ®ï¼ˆå¤„ç†åˆ†é¡µï¼‰"""
        all_data = []
        offset = 0
        limit = 1000
        
        print(f"ğŸ“Š å¼€å§‹è·å–è¡¨ {table_name} çš„æ‰€æœ‰æ•°æ®...")
        
        while True:
            batch_data = self.fetch_table_data(table_name, limit, offset, order_by)
            
            if not batch_data:
                break
                
            all_data.extend(batch_data)
            
            if len(batch_data) < limit:
                # å·²ç»è·å–å®Œæ‰€æœ‰æ•°æ®
                break
                
            offset += limit
            time.sleep(0.5)  # é¿å…è¯·æ±‚è¿‡å¿«
        
        print(f"ğŸ‰ è¡¨ {table_name} æ€»å…±è·å– {len(all_data)} æ¡è®°å½•")
        return all_data

    def crawl_categories(self):
        """è·å–æ‰€æœ‰åˆ†ç±»"""
        print("\n" + "="*50)
        print("ğŸ“‚ å¼€å§‹è·å–æ•…äº‹åˆ†ç±»...")
        print("="*50)
        
        self.categories = self.get_all_table_data('story_category', 'id')
        
        if self.categories:
            print(f"ğŸ“‹ åˆ†ç±»åˆ—è¡¨:")
            for category in self.categories[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
                print(f"   - ID: {category.get('id')}, åç§°: {category.get('name', 'N/A')}")
            
            if len(self.categories) > 10:
                print(f"   ... è¿˜æœ‰ {len(self.categories) - 10} ä¸ªåˆ†ç±»")
        
        return self.categories

    def crawl_stories(self):
        """è·å–æ‰€æœ‰æ•…äº‹ä¸»è¦ä¿¡æ¯"""
        print("\n" + "="*50)
        print("ğŸ“š å¼€å§‹è·å–æ•…äº‹ä¸»è¦ä¿¡æ¯...")
        print("="*50)
        
        self.stories = self.get_all_table_data('story_main', 'id')
        
        if self.stories:
            print(f"ğŸ“– æ•…äº‹åˆ—è¡¨é¢„è§ˆ:")
            for story in self.stories[:5]:  # æ˜¾ç¤ºå‰5ä¸ª
                print(f"   - ID: {story.get('id')}")
                print(f"     æ ‡é¢˜: {story.get('title', 'N/A')}")
                print(f"     åˆ†ç±»ID: {story.get('category_id', 'N/A')}")
                print(f"     åˆ›å»ºæ—¶é—´: {story.get('created_at', 'N/A')}")
                print()
            
            if len(self.stories) > 5:
                print(f"   ... è¿˜æœ‰ {len(self.stories) - 5} ä¸ªæ•…äº‹")
        
        return self.stories

    def crawl_story_contents(self):
        """è·å–æ‰€æœ‰æ•…äº‹å†…å®¹"""
        print("\n" + "="*50)
        print("ğŸ“ å¼€å§‹è·å–æ•…äº‹å†…å®¹...")
        print("="*50)
        
        self.story_contents = self.get_all_table_data('story_content', 'story_id')
        
        if self.story_contents:
            print(f"ğŸ“„ å†…å®¹åˆ—è¡¨é¢„è§ˆ:")
            for content in self.story_contents[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                story_id = content.get('story_id')
                content_text = content.get('content', '')
                print(f"   - æ•…äº‹ID: {story_id}")
                print(f"     å†…å®¹é•¿åº¦: {len(content_text)} å­—ç¬¦")
                print(f"     å†…å®¹é¢„è§ˆ: {content_text[:100]}...")
                print()
            
            if len(self.story_contents) > 3:
                print(f"   ... è¿˜æœ‰ {len(self.story_contents) - 3} ä¸ªæ•…äº‹å†…å®¹")
        
        return self.story_contents

    def save_to_json(self):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        print("\nğŸ’¾ ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶...")
        
        # ä¿å­˜åˆ†ç±»
        with open('supabase_categories.json', 'w', encoding='utf-8') as f:
            json.dump(self.categories, f, ensure_ascii=False, indent=2)
        print(f"âœ… åˆ†ç±»æ•°æ®å·²ä¿å­˜: supabase_categories.json ({len(self.categories)} æ¡)")
        
        # ä¿å­˜æ•…äº‹ä¸»è¦ä¿¡æ¯
        with open('supabase_stories.json', 'w', encoding='utf-8') as f:
            json.dump(self.stories, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•…äº‹æ•°æ®å·²ä¿å­˜: supabase_stories.json ({len(self.stories)} æ¡)")
        
        # ä¿å­˜æ•…äº‹å†…å®¹
        with open('supabase_story_contents.json', 'w', encoding='utf-8') as f:
            json.dump(self.story_contents, f, ensure_ascii=False, indent=2)
        print(f"âœ… æ•…äº‹å†…å®¹å·²ä¿å­˜: supabase_story_contents.json ({len(self.story_contents)} æ¡)")

    def save_to_sqlite(self):
        """ä¿å­˜æ•°æ®åˆ°SQLiteæ•°æ®åº“"""
        print("\nğŸ’¾ ä¿å­˜æ•°æ®åˆ°SQLiteæ•°æ®åº“...")
        
        conn = sqlite3.connect('supabase_complete_stories.db')
        cursor = conn.cursor()
        
        try:
            # åˆ›å»ºåˆ†ç±»è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS categories (
                    id INTEGER PRIMARY KEY,
                    name TEXT,
                    description TEXT,
                    created_at TEXT,
                    updated_at TEXT
                )
            ''')
            
            # åˆ›å»ºæ•…äº‹è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS stories (
                    id INTEGER PRIMARY KEY,
                    title TEXT,
                    category_id INTEGER,
                    excerpt TEXT,
                    reading_time INTEGER,
                    created_at TEXT,
                    updated_at TEXT,
                    FOREIGN KEY (category_id) REFERENCES categories (id)
                )
            ''')
            
            # åˆ›å»ºæ•…äº‹å†…å®¹è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS story_contents (
                    id INTEGER PRIMARY KEY,
                    story_id INTEGER,
                    content TEXT,
                    created_at TEXT,
                    updated_at TEXT,
                    FOREIGN KEY (story_id) REFERENCES stories (id)
                )
            ''')
            
            # æ’å…¥åˆ†ç±»æ•°æ®
            if self.categories:
                cursor.execute('DELETE FROM categories')  # æ¸…ç©ºæ—§æ•°æ®
                for category in self.categories:
                    cursor.execute('''
                        INSERT INTO categories (id, name, description, created_at, updated_at)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (
                        category.get('id'),
                        category.get('name'),
                        category.get('description'),
                        category.get('created_at'),
                        category.get('updated_at')
                    ))
                print(f"âœ… åˆ†ç±»æ•°æ®å·²æ’å…¥: {len(self.categories)} æ¡")
            
            # æ’å…¥æ•…äº‹æ•°æ®
            if self.stories:
                cursor.execute('DELETE FROM stories')  # æ¸…ç©ºæ—§æ•°æ®
                for story in self.stories:
                    cursor.execute('''
                        INSERT INTO stories (id, title, category_id, excerpt, reading_time, created_at, updated_at)
                        VALUES (?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        story.get('id'),
                        story.get('title'),
                        story.get('category_id'),
                        story.get('excerpt'),
                        story.get('reading_time'),
                        story.get('created_at'),
                        story.get('updated_at')
                    ))
                print(f"âœ… æ•…äº‹æ•°æ®å·²æ’å…¥: {len(self.stories)} æ¡")
            
            # æ’å…¥æ•…äº‹å†…å®¹æ•°æ®
            if self.story_contents:
                cursor.execute('DELETE FROM story_contents')  # æ¸…ç©ºæ—§æ•°æ®
                for content in self.story_contents:
                    cursor.execute('''
                        INSERT INTO story_contents (id, story_id, content, created_at, updated_at)
                        VALUES (?, ?, ?, ?, ?)
                    ''', (
                        content.get('id'),
                        content.get('story_id'),
                        content.get('content'),
                        content.get('created_at'),
                        content.get('updated_at')
                    ))
                print(f"âœ… æ•…äº‹å†…å®¹å·²æ’å…¥: {len(self.story_contents)} æ¡")
            
            conn.commit()
            print("âœ… SQLiteæ•°æ®åº“ä¿å­˜å®Œæˆ: supabase_complete_stories.db")
            
        except Exception as e:
            print(f"âŒ SQLiteä¿å­˜å¤±è´¥: {e}")
            conn.rollback()
        finally:
            conn.close()

    def generate_report(self):
        """ç”Ÿæˆçˆ¬å–æŠ¥å‘Š"""
        report = {
            "crawl_time": datetime.now().isoformat(),
            "supabase_url": self.supabase_url,
            "tables_crawled": {
                "story_category": len(self.categories),
                "story_main": len(self.stories),
                "story_content": len(self.story_contents)
            },
            "total_records": len(self.categories) + len(self.stories) + len(self.story_contents),
            "files_generated": [
                "supabase_categories.json",
                "supabase_stories.json", 
                "supabase_story_contents.json",
                "supabase_complete_stories.db",
                "supabase_crawl_report.json"
            ]
        }
        
        # æ·»åŠ åˆ†ç±»ç»Ÿè®¡
        if self.categories:
            report["category_stats"] = {
                "total_categories": len(self.categories),
                "category_names": [cat.get('name') for cat in self.categories[:10]]
            }
        
        # æ·»åŠ æ•…äº‹ç»Ÿè®¡
        if self.stories:
            category_counts = {}
            for story in self.stories:
                cat_id = story.get('category_id')
                category_counts[cat_id] = category_counts.get(cat_id, 0) + 1
            
            report["story_stats"] = {
                "total_stories": len(self.stories),
                "stories_by_category": category_counts,
                "avg_reading_time": sum(s.get('reading_time', 0) for s in self.stories) / len(self.stories) if self.stories else 0
            }
        
        # ä¿å­˜æŠ¥å‘Š
        with open('supabase_crawl_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report

    def run_full_crawl(self):
        """æ‰§è¡Œå®Œæ•´çˆ¬å–"""
        print("ğŸš€ å¼€å§‹Supabaseå®Œæ•´æ•°æ®çˆ¬å–...")
        print("="*60)
        
        start_time = time.time()
        
        try:
            # 1. è·å–åˆ†ç±»
            self.crawl_categories()
            
            # 2. è·å–æ•…äº‹
            self.crawl_stories()
            
            # 3. è·å–æ•…äº‹å†…å®¹
            self.crawl_story_contents()
            
            # 4. ä¿å­˜æ•°æ®
            self.save_to_json()
            self.save_to_sqlite()
            
            # 5. ç”ŸæˆæŠ¥å‘Š
            report = self.generate_report()
            
            end_time = time.time()
            duration = end_time - start_time
            
            print("\n" + "="*60)
            print("ğŸ‰ Supabaseæ•°æ®çˆ¬å–å®Œæˆï¼")
            print("="*60)
            print(f"â±ï¸  æ€»è€—æ—¶: {duration:.2f} ç§’")
            print(f"ğŸ“Š æ€»è®°å½•æ•°: {report['total_records']}")
            print(f"ğŸ“‚ åˆ†ç±»æ•°: {len(self.categories)}")
            print(f"ğŸ“š æ•…äº‹æ•°: {len(self.stories)}")
            print(f"ğŸ“ å†…å®¹æ•°: {len(self.story_contents)}")
            print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
            for file in report['files_generated']:
                print(f"   - {file}")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            return False

if __name__ == "__main__":
    crawler = SupabaseDirectCrawler()
    success = crawler.run_full_crawl()
    
    if success:
        print("\nâœ… æ‰€æœ‰æ•…äº‹æ•°æ®å·²æˆåŠŸè·å–ï¼")
    else:
        print("\nâŒ çˆ¬å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®ã€‚")